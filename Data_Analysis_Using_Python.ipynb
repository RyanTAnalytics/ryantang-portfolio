{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis of Brazilian E-Commerce Dataset with Python (Pandas)\n",
    "\n",
    "\n",
    "Let's use Pandas to explore and analyze some data. The public dataset was taken from the [Brazilian e-commerce platform, Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce). It has been filtered to only contain 2017 data.\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "Olist, a Brazilian company providing a marketplace integration platform for small and medium-sized businesses, seeks to leverage data analytics to optimize its sales strategies. The company has access to a comprehensive public dataset containing various aspects of their e-commerce activities, including customer information, order details, product listings, seller performance, and more. \n",
    "\n",
    "*Objectives:*\n",
    "1. Analyze the changes in total sales over time in 2017.\n",
    "2. Evaluate the performance of different categories to identify top-selling items and underperformers.\n",
    "3. Identify which regions generate the most sales to inform sales strategies. \n",
    "4. Assess seller performance to provide insights and recommendations for improving seller ratings and sales.\n",
    "\n",
    "*Research Questions:*\n",
    "1. How do total sales fluctuate over the year and week?\n",
    "2. Which categories of products are most popular and least popular? \n",
    "3. Which regions generate the most sales from customers?\n",
    "4. Which regions contain the highest performing sellers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1: Data Exploration\n",
    "\n",
    "Let's start by only looking at the `olist_order_items_2017.csv` data and explore its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reading in the data\n",
    "order_items = pd.read_csv('olist_order_items_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 1.1: Inspecting the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using head() to show the first 5 rows\n",
    "order_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tails() to show the last 5 rows\n",
    "order_items.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using shape to view number of rows and columns\n",
    "order_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using info to view column names/types and more\n",
    "order_items.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 1.2: Data Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using describe to view descriptive statistics\n",
    "order_items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using unique() to look at categorical values\n",
    "order_items['product_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using value_counts() to reveal product categories with highest sales\n",
    "order_items['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using value_counts() to reveal sellers with highest sales\n",
    "order_items['seller_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2: Data Transformation & Wrangling\n",
    "\n",
    "We have looked at the ordered items in the Olist e-commerce platform. Let's look at when these orders were placed.\n",
    "\n",
    "Let's look into the `olist_orders_2017.csv` dataset which seems to contain some missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the orders dataset\n",
    "orders = pd.read_csv('olist_orders_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking its info\n",
    "orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2.1: Identifying Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isnull() and sum() to count the number of missing values\n",
    "orders.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2.2: Filling in Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy to protect original data\n",
    "orders_copy = orders.copy()\n",
    "\n",
    "# Remove the rows which really have all missing values\n",
    "orders.dropna(how='all', inplace=True)\n",
    "\n",
    "# Fill in missing values \n",
    "orders.fillna(value={'order_status':orders['order_status'].mode()[0],\n",
    "                     'approval_lag':orders['approval_lag'].median()}, inplace=True)\n",
    "\n",
    "#Check nulls\n",
    "orders.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the original DataFrame if needed\n",
    "# orders =  orders_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2.2: Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename functions to be more mnemonic \n",
    "old_and_new = {'product_category':'category', 'shipping_limit_date':'ship_date'}\n",
    "\n",
    "order_items.rename(columns=old_and_new, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2.3: Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the irrelevant columns from order_items\n",
    "order_items.drop(['shipping_limit_date', 'freight_value'], axis='columns', inplace=True)\n",
    "\n",
    "# drop the irrelevant columns from orders\n",
    "orders.drop(['order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date'], \n",
    "            axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2.4: Merging Data\n",
    "\n",
    "Since we need retain **all** the data in the product_ids column within the order_items dataset, we will use a `RIGHT JOIN` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to record products ordered, category and when they were purchased.\n",
    "ordered_products = pd.merge(orders, order_items, how='right', on='order_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis to meet the objectives effectively, we would also need to bring in data from customer and seller data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the datasets\n",
    "customers = pd.read_csv('olist_customers_2017.csv')\n",
    "sellers = pd.read_csv('olist_sellers_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns for customers\n",
    "customers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we want to retain the product_ids column. Therefore, we will use a `LEFT JOIN` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the ordered_products with customers to find out who the product is sold to\n",
    "merged_df = pd.merge(ordered_products, customers, how='left', on='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use a `LEFT JOIN` here as well to retain the product_ids column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the newly merged DataFrame with sellers to find out who the product is sold by\n",
    "merged_df = pd.merge(merged_df, sellers, how='left', on='seller_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the merged dataset\n",
    "merged_df\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2.5: Cleaning Up\n",
    "\n",
    "After merging, we seem to have a wide variety of columns, many of which are irrelevant to our objective or are not needed for our analysis. Therefore, we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns and save into a new DataFrame\n",
    "products_sold = merged_df.drop(['customer_id', 'order_approved_at', 'approval_lag', 'order_item_id',\n",
    "                                 'seller_id', 'customer_unique_id', 'customer_zip_code_prefix', \n",
    "                                  'customer_city', 'seller_zip_code_prefix', 'seller_city'],axis='columns')\n",
    "\n",
    "# Looking into the new dataframe\n",
    "products_sold.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data types\n",
    "products_sold.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the datatype, it seems that the `order_purchase_timestamp` has been incorrectly assigned as an a categorical object. Let's convert it into the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DateTime using to_datetime() method\n",
    "products_sold['order_purchase_timestamp']= pd.to_datetime(products_sold['order_purchase_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Date Information**\n",
    "\n",
    "Now we can use DateTime functions to get more information about the purchase date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month of Order\n",
    "products_sold['order_month'] = products_sold['order_purchase_timestamp'].dt.month\n",
    "\n",
    "# Day of Week \n",
    "products_sold['order_day'] = products_sold['order_purchase_timestamp'].dt.weekday\n",
    "\n",
    "# Day Name\n",
    "products_sold['order_day_name'] = products_sold['order_purchase_timestamp'].dt.day_name()\n",
    "\n",
    "# Hour of the day\n",
    "products_sold['order_hour'] = products_sold['order_purchase_timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our analysis more fruitful and straightforward (clearer and less complicated), we can further breakdown the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values in category as 'Other'\n",
    "products_sold['product_category'].fillna('Other', inplace=True)\n",
    "\n",
    "# Rename category as subcategory\n",
    "products_sold.rename(columns={'product_category':'subcategory'}, inplace=True)\n",
    "\n",
    "# Define a function to break down the categories further\n",
    "def set_category(sub_category):\n",
    "    if 'fashion' in sub_category:\n",
    "        return 'fashion'\n",
    "    elif 'furniture' in sub_category:\n",
    "        return 'furniture'\n",
    "    elif 'tool' in sub_category:\n",
    "        return 'tools'\n",
    "    elif 'home' in sub_category or sub_category in ['housewares','bed_bath_table']:\n",
    "        return 'home'\n",
    "    elif sub_category in ['consoles_games', 'computers', 'computers_accessories', 'electronics', 'tablets_printing_image']:\n",
    "        return 'electronics'\n",
    "    elif sub_category in ['audio','music','musical_instruments']:\n",
    "        return 'audio'\n",
    "    elif sub_category in ['baby','toys', 'cool_stuff']:\n",
    "        return 'kids'\n",
    "    else:\n",
    "        return 'other category'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and create a new column based on the subcategory\n",
    "products_sold['category'] = products_sold['subcategory'].apply(set_category)\n",
    "\n",
    "# Check the unique values in the category column\n",
    "products_sold['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts\n",
    "products_sold['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the merged data to a new file called products_sold_2017.csv, without adding an index column\n",
    "products_sold.to_csv('products_sold_2017.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 3: Data Analysis\n",
    "\n",
    "Great! Our data is now properly explored, transformed, and wrangled.\n",
    "\n",
    "Now it is time to do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the categories values, including missing values\n",
    "products_sold['category'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 3.1: Grouping and Summarizing\n",
    "\n",
    "As you can see above, we have been able to count the number of order items by each category. \n",
    "\n",
    "We can summarize the data by category, sub_category, month, day of order and even time of order! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by category, then count the number of order_ids for each category\n",
    "products_sold.groupby('category')['order_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary values of sales by month\n",
    "products_sold.groupby('order_month')['price'].agg(['count', 'sum','mean','min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary values of sales by day \n",
    "products_sold.groupby('order_day_name')['price'].agg(['count', 'sum','mean','min','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 4: Data Visualization\n",
    "\n",
    "Visualization 1: Total Sales by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = products_sold.groupby('order_month')['price'].sum()\n",
    "plotdata.plot(title='Total Sales by Month in 2017')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 2: Fluctuations of Total Sales throughout the Week by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = products_sold.groupby(['order_month', 'order_day_name'])['price'].count().unstack()\n",
    "\n",
    "# Reorder the columns to match the order of the days of the week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plotdata = plotdata[day_order]\n",
    "\n",
    "# Plot the data\n",
    "plotdata.plot(title=\"Fluctuations of Total Sales throughout the Week by Month\")\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend(title='Day of Week', labels=day_order)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 3: Total Sales by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = products_sold.groupby(['category'])['price'].count()\n",
    "plotdata.plot(kind='barh', title='Total Sales by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 4: Top 10 Performing States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = products_sold.groupby(['customer_state'])['price'].sum()\n",
    "plotdata.sort_values().tail(10).plot(kind='barh', title='Top 10 Performing States')\n",
    "plt.xlabel('Total Sales')\n",
    "plt.ylabel('Customer State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 5: Top 10 Performing Sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = products_sold.groupby(['seller_state'])['price'].sum()\n",
    "plotdata.sort_values().tail(10).plot(kind='barh', title='Top 10 Performing Sellers')\n",
    "plt.xlabel('Total Sales')\n",
    "plt.ylabel('Seller State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 5: Conclusions & Suggestions\n",
    "\n",
    "1. How do total sales fluctuate over the year and week?\n",
    "- Based on Visualization 1, it seems that total sales starts out low and increases rather gradually over the year. However, there is a large spike at the end of the year, peaking in November. A similar trend can be seen in Visualization 2, where total sales gradually increase over the week, generally peaking on Friday for all months. There is a significant spike on Friday in November, where total sales almost double that of other days of the week.\n",
    "- Suggestions:\n",
    "    - Given the spike in November, particularly on Fridays, it’s clear that Black Friday and Cyber Monday are crucial periods. Plan extensive marketing campaigns and promotions targeting these dates.\n",
    "    - Extend special promotions through December to maintain the momentum from the November peak, focusing on holiday shopping.\n",
    "    - Implement regular “Flash Sale Fridays” with special discounts and promotions to capitalize on the natural peak in sales.\n",
    "    - Increase inventory for high-demand products well in advance of November to avoid stockouts.\n",
    "    \n",
    "2. Which categories of products are most popular and least popular?\\\n",
    "- Based on Visualization 3, it seems that products in the home category brings in the most sales individually,followed by kids, furniture, and electronics respectively. Though, the categories could be broken down further to reveal more interesting insights.\n",
    "- Suggestions:\n",
    "    - Highlight home category products prominently on the homepage and in marketing campaigns.\n",
    "    - Align home category promotions with seasonal events such as spring cleaning, back-to-school, and holiday decorating\n",
    "    - Ensure adequate stock levels for high-demand products in the home, kids, furniture, and electronics categories, especially during peak sales periods.\n",
    "3. Which regions generate the most sales from customers?\n",
    "- Based on Visualization 4, it seems that highest total sales come overwhelmingly from customers in Sao Paulo compared to other states in Brazil like Rio de Janeiro and Minas Gerais. This is not surprising given Sao Paulo is the most populous and developed state in Brazil\n",
    "- Suggestions:\n",
    "    - Increase investment in digital and traditional advertising specifically targeting São Paulo. Utilize platforms like Google Ads, Facebook, and Instagram with geo-targeting to reach São Paulo residents.\n",
    "    - Consider establishing local warehouses or distribution centers in São Paulo to reduce delivery times and improve logistics efficiency.\n",
    "4. Which regions contain the highest performing sellers?\n",
    "- Based on Visualization 5, highest performing sellers in terms of total sales also seem to be from Sao Paulo, followed by Parana and Minas Gerais.\n",
    "- Suggestions:\n",
    "    - Work with logistics partners to offer more efficient and cost-effective shipping solutions for sellers, especially in high-performing regions like São Paulo, Paraná, and Minas Gerais.\n",
    "    - Provide tools and training for better inventory management, ensuring sellers can meet demand without overstocking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
